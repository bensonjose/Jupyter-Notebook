{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"I believe this would help the reader understand how tokenization \\\n",
        "        works. as well as realize its importance.\"\n",
        "        \n",
        "sents = (sent_tokenize(text))        \n",
        "print(sents)\n",
        "\n",
        "print (word_tokenize(text))\n",
        "\n",
        "words = [word_tokenize(sent) for sent in sents]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJgtaTiITT3F",
        "outputId": "d1649910-f2f5-4598-bc4d-e611e3692bfa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I believe this would help the reader understand how tokenization         works.', 'as well as realize its importance.']\n",
            "['I', 'believe', 'this', 'would', 'help', 'the', 'reader', 'understand', 'how', 'tokenization', 'works', '.', 'as', 'well', 'as', 'realize', 'its', 'importance', '.']\n",
            "[['I', 'believe', 'this', 'would', 'help', 'the', 'reader', 'understand', 'how', 'tokenization', 'works', '.'], ['as', 'well', 'as', 'realize', 'its', 'importance', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TOQ3hdxhVyUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "apWXtceuVvXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"I believe this would help the reader understand how tokenization \\\n",
        "        works. as well as realize its importance (text) .\"\n",
        "        \n",
        "custom_list = set(stopwords.words('english')+list(punctuation))        \n",
        "        \n",
        "word_list = [word for word in word_tokenize(text) if word not in custom_list]\n",
        "print(word_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyvVkA7-TLMY",
        "outputId": "cd9fe473-7241-422a-f916-a72e5fe25052"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'believe', 'would', 'help', 'reader', 'understand', 'tokenization', 'works', 'well', 'realize', 'importance', 'text']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#N-grams\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "\n",
        "word_list = ['I', 'believe', 'would', 'help', 'reader', 'understand', \\\n",
        "             'tokenization', 'works', 'well', 'realize', 'importance', 'text']\n",
        "\n",
        "finde = BigramCollocationFinder.from_words(word_list)\n",
        "print(finde.ngram_fd.items())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgZ3w_LSU0j3",
        "outputId": "e93f83a5-b25e-4cf6-885a-f4287e680949"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([(('I', 'believe'), 1), (('believe', 'would'), 1), (('would', 'help'), 1), (('help', 'reader'), 1), (('reader', 'understand'), 1), (('understand', 'tokenization'), 1), (('tokenization', 'works'), 1), (('works', 'well'), 1), (('well', 'realize'), 1), (('realize', 'importance'), 1), (('importance', 'text'), 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "#Import Libraries\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "\n",
        "new_text = \"It is important to by very pythonly while you are pythoning\\\n",
        "             with python. All pythoners have pythoned poorly at least once.\"\n",
        "             \n",
        "l_s =  LancasterStemmer()\n",
        "stem_lan =  [l_s.stem(word) for word in word_tokenize(new_text)]\n",
        "print(stem_lan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8XnGpm0VAjy",
        "outputId": "263b1246-558e-49ec-ad96-f7f00f358b2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['it', 'is', 'import', 'to', 'by', 'very', 'python', 'whil', 'you', 'ar', 'python', 'with', 'python', '.', 'al', 'python', 'hav', 'python', 'poor', 'at', 'least', 'ont', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Sense Disambiguation\n",
        "#Import Libraries\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "  \n",
        "from nltk.corpus import wordnet\n",
        "for ss in wordnet.synsets('mouse'):\n",
        "    print(ss, ss.definition())\n",
        "    \n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "context_1 = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"), \"bass\")\n",
        "print(context_1, context_1.definition())\n",
        "\n",
        "context_2 = lesk(word_tokenize(\"The sea bass really very hard to catch\"), \"bass\")\n",
        "print(context_2, context_2.definition())\n",
        "\n",
        "context_3 = lesk(word_tokenize(\"My mouse is not working, need to change it\"), \"mouse\")\n",
        "print(context_3, context_3.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sawJS8ysVGCB",
        "outputId": "20382fd5-5b80-428c-b397-0ed74717a9cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('mouse.n.01') any of numerous small rodents typically resembling diminutive rats having pointed snouts and small ears on elongated bodies with slender usually hairless tails\n",
            "Synset('shiner.n.01') a swollen bruise caused by a blow to the eye\n",
            "Synset('mouse.n.03') person who is quiet or timid\n",
            "Synset('mouse.n.04') a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n",
            "Synset('sneak.v.01') to go stealthily or furtively\n",
            "Synset('mouse.v.02') manipulate the mouse of a computer\n",
            "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
            "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
            "Synset('mouse.n.04') a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tk6XXob9WAbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "corpus = [\n",
        "     'This is the first document from heaven',\n",
        "     'but the second document is from mars',\n",
        "     'And this is the third one from nowhere',\n",
        "     'Is this the first document from nowhere?',\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({'Text':corpus})\n",
        "print(df)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_v = CountVectorizer()\n",
        "X = count_v.fit_transform(df.Text).toarray()\n",
        "print(count_v.get_feature_names())\n",
        "\n",
        "print(X)\n",
        "print(count_v.vocabulary_)\n",
        "\n",
        "count_v = CountVectorizer(stop_words=['this','is'])\n",
        "X = count_v.fit_transform(df.Text).toarray()\n",
        "\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OLmVQ44VK0C",
        "outputId": "4376b293-df89-4106-a857-b3e5f29052dd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       Text\n",
            "0    This is the first document from heaven\n",
            "1      but the second document is from mars\n",
            "2    And this is the third one from nowhere\n",
            "3  Is this the first document from nowhere?\n",
            "['and', 'but', 'document', 'first', 'from', 'heaven', 'is', 'mars', 'nowhere', 'one', 'second', 'the', 'third', 'this']\n",
            "[[0 0 1 1 1 1 1 0 0 0 0 1 0 1]\n",
            " [0 1 1 0 1 0 1 1 0 0 1 1 0 0]\n",
            " [1 0 0 0 1 0 1 0 1 1 0 1 1 1]\n",
            " [0 0 1 1 1 0 1 0 1 0 0 1 0 1]]\n",
            "{'this': 13, 'is': 6, 'the': 11, 'first': 3, 'document': 2, 'from': 4, 'heaven': 5, 'but': 1, 'second': 10, 'mars': 7, 'and': 0, 'third': 12, 'one': 9, 'nowhere': 8}\n",
            "[[0 0 1 1 1 1 0 0 0 0 1 0]\n",
            " [0 1 1 0 1 0 1 0 0 1 1 0]\n",
            " [1 0 0 0 1 0 0 1 1 0 1 1]\n",
            " [0 0 1 1 1 0 0 1 0 0 1 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TD-IDF\n",
        "#Import Libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "     'This is the first document from heaven',\n",
        "     'but the second document is from mars',\n",
        "     'And this is the third one from nowhere',\n",
        "     'Is this the first document from nowhere?',\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0avIFLJBVZ7P",
        "outputId": "e3733bcc-932c-40f1-fd56-3af91b856efe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'this': 13, 'is': 6, 'the': 11, 'first': 3, 'document': 2, 'from': 4, 'heaven': 5, 'but': 1, 'second': 10, 'mars': 7, 'and': 0, 'third': 12, 'one': 9, 'nowhere': 8}\n",
            "[1.91629073 1.91629073 1.22314355 1.51082562 1.         1.91629073\n",
            " 1.         1.91629073 1.51082562 1.91629073 1.91629073 1.\n",
            " 1.91629073 1.22314355]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hashing\n",
        "#Import Libraries\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import pandas as pd\n",
        "corpus = [\n",
        "     'This is the first document from heaven',\n",
        "     'but the second document is from mars',\n",
        "     'And this is the third one from nowhere',\n",
        "     'Is this the first document from nowhere?',\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({'Text':corpus})\n",
        "\n",
        "hash_v = HashingVectorizer(n_features=15, norm=None, alternate_sign=True)\n",
        "print(hash_v.fit_transform(df.Text).toarray())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgB756xxVh1u",
        "outputId": "b6073344-d6ca-4191-bb88-0f65a0e0826f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0. -1.  1.  0.  0.]\n",
            " [ 0.  0.  0. -1.  0. -1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -1.  0. -1.  1.  0.  0.  0.  0.  0.  2.  0. -1.]\n",
            " [ 0.  0. -1. -1.  0.  0.  0.  0.  0.  0.  0. -1.  2.  0.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9nX9AZcVpNu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}